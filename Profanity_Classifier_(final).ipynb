{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 욕설 분류기\n",
    "---------\n",
    "## CNN을 이용한 인터넷 채팅상의 욕설 분류 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "from konlpy.tag import Kkma\n",
    "import hgtk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "## 1) 전처리\n",
    "### 사용할 초성, 중성, 종성 리스트 합본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jong_list = hgtk.const.JONG[1:]\n",
    "jamo = (hgtk.const.CHO,hgtk.const.JOONG,tuple(jong_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(raw_sentence):\n",
    "    noun_and_unknown = []\n",
    "    for word in kkma.pos(raw_sentence):\n",
    "        if word[1] == \"NNG\" or word[1] == \"UN\":\n",
    "            noun_and_unknown.append(word[0])\n",
    "    decompose_sentence = \"\".join(noun_and_unknown)\n",
    "    count = len(decompose_sentence)\n",
    "    range_list = [(0,5)]\n",
    "    if count > 5:\n",
    "        i = 0\n",
    "        while (i+5) < count:\n",
    "            i += 2\n",
    "            range_list.append((i,i+5))\n",
    "    decom_list = []\n",
    "    for s,t in range_list:\n",
    "        decom_list.append(hgtk.text.decompose(decompose_sentence[s:t]))\n",
    "    return decom_list, len(decom_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a,b= process_sentence(\"이 씹새끼가 어디서 약을 팔어 씹ㅏㄹ새끼야\")  \n",
    "print(a,b)\n",
    "\n",
    "------\n",
    "['ㅆㅣㅂᴥㅅㅐᴥㄲㅣᴥㅇㅑㄱᴥㅍㅏㄹᴥ',  \n",
    "'ㄲㅣᴥㅇㅑㄱᴥㅍㅏㄹᴥㅆㅣㅂᴥㅏᴥ',  \n",
    "'ㅍㅏㄹᴥㅆㅣㅂᴥㅏᴥㄹᴥㅅㅐᴥ',  \n",
    "'ㅏᴥㄹᴥㅅㅐᴥㄲㅣᴥ'] 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def let2mat(processed):\n",
    "    result_np = np.zeros((5,3))\n",
    "    i, index = 0, 0\n",
    "    for c in processed:\n",
    "        if c == 'ᴥ':\n",
    "            index += 1\n",
    "            i = 0\n",
    "            continue\n",
    "        if (c not in jamo[i]):\n",
    "            i += 1\n",
    "            if (c not in jamo[i]):\n",
    "                i += 1\n",
    "        result_np[index,i] = jamo[i].index(c) + 1 #공백과 첫번째 인덱스를 구분하기 위해서\n",
    "        i += 1\n",
    "    return result_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(raw_sentence):\n",
    "    a,b = process_sentence(raw_sentence)\n",
    "    result = let2mat(a[0])\n",
    "    for i in range(1,b):\n",
    "        if result.ndim == 2:\n",
    "            result = np.r_[[result],[let2mat(a[i])]]\n",
    "            continue\n",
    "        result = np.r_[result,[let2mat(a[i])]]\n",
    "    if result.ndim == 2:\n",
    "        return np.r_[[result]],b\n",
    "    return result,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a,b = process_sentence(\"이 씹새끼가 어디서 약을 팔어 씹ㅏㄹ새끼야\")\n",
    "print(a,b)\n",
    "process(\"이 씹새끼가 어디서 약을 팔어 씹ㅏㄹ새끼야\")\n",
    "\n",
    "-----\n",
    "['ㅆㅣㅂᴥㅅㅐᴥㄲㅣᴥㅇㅑㄱᴥㅍㅏㄹᴥ', 'ㄲㅣᴥㅇㅑㄱᴥㅍㅏㄹᴥㅆㅣㅂᴥㅏᴥ', 'ㅍㅏㄹᴥㅆㅣㅂᴥㅏᴥㄹᴥㅅㅐᴥ', 'ㅏᴥㄹᴥㅅㅐᴥㄲㅣᴥ'] 4  \n",
    "\n",
    "      [[[11., 21., 17.],  \n",
    "        [10.,  2.,  0.],  \n",
    "        [ 2., 21.,  0.],  \n",
    "        [12.,  3.,  1.],  \n",
    "        [18.,  1.,  8.]],  \n",
    "        \n",
    "       [[ 2., 21.,  0.],  \n",
    "        [12.,  3.,  1.],  \n",
    "        [18.,  1.,  8.],  \n",
    "        [11., 21., 17.],  \n",
    "        [ 0.,  1.,  0.]],  \n",
    "        \n",
    "       [[18.,  1.,  8.],  \n",
    "        [11., 21., 17.],  \n",
    "        [ 0.,  1.,  0.],  \n",
    "        [ 6.,  0.,  0.],  \n",
    "        [10.,  2.,  0.]],  \n",
    "        \n",
    "       [[ 0.,  1.,  0.],  \n",
    "        [ 6.,  0.,  0.],  \n",
    "        [10.,  2.,  0.],  \n",
    "        [ 2., 21.,  0.],  \n",
    "        [ 0.,  0.,  0.]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas로 파일 읽어들이고 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dcnate.csv',names = ['text','label'],encoding = 'cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "row_data = list(df['text'])\n",
    "label= list(df['label'])\n",
    "Y_label = []\n",
    "\n",
    "X_data, count = process(row_data[0])\n",
    "for _ in range(0,count):\n",
    "    Y_label.append(label[0])\n",
    "\n",
    "for idx in range(1,len(row_data)):\n",
    "    X, count = process(row_data[idx])\n",
    "    X_data = np.r_[X_data,X]\n",
    "    for _ in range(0,count):\n",
    "        Y_label.append(label[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data :1334개 label : 1334개\n"
     ]
    }
   ],
   "source": [
    "print(\"data :{}개 label : {}개\".format(len(X_data),len(Y_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input, 15개의 값을 가지며 n개의 이미지이다.\n",
    "X = tf.placeholder(tf.float32, [None,5,3]) \n",
    "# input 인식하기 위해 reshape을 해준다. 5*3의 행렬이며, 개수는 n개이므로 -1\n",
    "X_mat = tf.reshape(X, [-1,5,3,1]) \n",
    "# output\n",
    "Y = tf.placeholder(tf.float32, [None,1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\profanity_classifier_TF\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# 3*3크기의 필터, 총 32개의 필터\n",
    "W1 = tf.Variable(tf.random_normal([3,3,1,32], stddev=0.1)) \n",
    "# conv2d 를 통과해도 5*3 크기를 가짐, 대신 32개의 필터이므로 총 32개의 결과가 생김\n",
    "L1 = tf.nn.conv2d(X_mat, W1, strides=[1,1,1,1], padding='SAME') \n",
    "L1 = tf.nn.relu(L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 2\n",
    "# 이번에는 64개의 필터\n",
    "W2 = tf.Variable(tf.random_normal([3,3,32,64], stddev = 0.1))\n",
    "# conv2d layer를 통과시키면, [?,5,3,64] 형태를 가짐\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1,1,1,1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "# 이후 쭉 펼친다.\n",
    "L2 = tf.reshape(L2, [-1,5 * 3 * 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-14-fe4bc26a745c>:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fully-connected layer\n",
    "W3 = tf.get_variable(\"W3\", shape=[5 * 3 * 64, 2],initializer = tf.contrib.layers.xavier_initializer())\n",
    "b = tf.Variable(tf.random_normal([2]))\n",
    "hypothesis = tf.matmul(L2, W3) + b\n",
    " \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(path, batch_size):\n",
    "    img, label, paths = [], [], []\n",
    "    for i in range(batch_size):\n",
    "        img.append()\n",
    "        label.append(int(path[0][1]))\n",
    "        paths.append(path.pop(0))\n",
    "        \n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning started. It takes sometimes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1334,) for Tensor 'Placeholder_5:0', which has shape '(?, 1)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-79a29a282d7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtotal_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mY_label\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mavg_cost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"%04d\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"cost =\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"{:.9f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavg_cost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\profanity_classifier_TF\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\profanity_classifier_TF\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m                              \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[1;32m-> 1128\u001b[1;33m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m   1129\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (1334,) for Tensor 'Placeholder_5:0', which has shape '(?, 1)'"
     ]
    }
   ],
   "source": [
    "#수정중 - by.이재용\n",
    "\n",
    "# init\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    " \n",
    "# train\n",
    "print('Learning started. It takes sometimes.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _, = sess.run([cost,optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "    print(\"Epoch:\",\"%04d\"%(epoch + 1),\"cost =\",\"{:.9f}\".format(avg_cost))\n",
    "print('Learning Finished!')\n",
    " \n",
    "# Test\n",
    "correct_prediction = tf.equal(tf.math.argmax(hypothesis, 1), tf.arg_max(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:',sess.run(accuracy,feed_dict={X: mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  9., 21.],\n",
       "        [ 7., 14.,  8.],\n",
       "        [19.,  1.,  0.],\n",
       "        [ 3.,  9.,  0.],\n",
       "        [ 0.,  0.,  0.]],\n",
       "\n",
       "       [[ 8.,  1.,  8.],\n",
       "        [ 7., 21.,  0.],\n",
       "        [10.,  1.,  0.],\n",
       "        [12., 21.,  8.],\n",
       "        [10.,  2.,  0.]],\n",
       "\n",
       "       [[10.,  1.,  0.],\n",
       "        [12., 21.,  8.],\n",
       "        [10.,  2.,  0.],\n",
       "        [ 2., 21.,  0.],\n",
       "        [10., 21.,  1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[10., 14.,  0.],\n",
       "        [ 1., 14.,  0.],\n",
       "        [ 2.,  9.,  8.],\n",
       "        [17.,  9., 21.],\n",
       "        [ 4., 19., 21.]],\n",
       "\n",
       "       [[ 2.,  9.,  8.],\n",
       "        [17.,  9., 21.],\n",
       "        [ 4., 19., 21.],\n",
       "        [10., 21., 16.],\n",
       "        [ 3.,  1.,  1.]],\n",
       "\n",
       "       [[ 4., 19., 21.],\n",
       "        [10., 21., 16.],\n",
       "        [ 3.,  1.,  1.],\n",
       "        [12., 21.,  0.],\n",
       "        [ 0.,  0.,  0.]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
